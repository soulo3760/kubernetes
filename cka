question 1 killer.sh
k alias kubectl
open firefox 
open two tabs
open kubernetes documentation

TASK ONE
writing contexts into a file 
kubectl config get-contexts
kubectl config get-context -o name
  kubectl config get-context -o name > /opt/contexts

  # write a command using kubectl 
  echo "kubectl config current-context" > /opt/course/context_default_kubectl.sh
  # write the same command without use of kubectl

  kube config file used to 
  
cat /opt/course/l/context_default_kubectl.sh
cat -/.kube/config | grep -I current-context

TASK 2
  execute this in a different context

kubectl config use-context k8s-c1-H
now lets create a pod
kubectl run -n default pod1 --image=httpd:2.4.41-alpine --dry-run=client -o yaml > 2.yaml
kubectl get nodes
#copy the name of the master node
#lets create the pod
kubectl create -f 2.yaml
#
kubectl get pods -n default pod1
kubectl get pods -n default pod1 -o wide 
#we should see master node 


TASK 3
kubectl config use-context k8s-c1-H
# lets see all the nodes running in this namespace 
kubectl get all -n project-c13
#to scale down on the replicas
kubectl scale statefulset -n project-c13 03db --replicas=1
#verfiy 
kubectl get pods -n project-c13 | grep -i 03db

TASK 4
Create a pod named ready if service exists
kubectl run ready-if-service-ready -n default --image=nginx:1.16.1-alpine --dry-run=client -o yaml
kubectl run ready-if-service-ready -n default --image=nginx:1.16.1-alpine --dry-run=client -o yaml > 4.yaml
#configure liveness prob
add this to the yaml under the containers specifications
vi 4.yamlre

 livenessprobe:
  exec:
    command: 
        - 'true'
readinessProbe:
 exec:
    command: 
        - 'sh'    #invoke sheell
        - '-c'    # command 
        - 'wget -T2 -0- https://services-am-i-ready:80'
vi 4.yaml
kubectl create -f 4.yaml
kubectl get pods -n default ready-if-service-ready
#if it is still not running for some reasons
lets describe the pod
kubectl describe pods -n default ready-if-service-ready
# it is not able to reach port 80 despite it running

## CREATE ANOTHER POD 
kubectl run am-i-ready -n default --image=nginx:1.16.1-alpine --labels='id=cross.server-ready'
#there is aother service existing with another pod
lets check on this 
kubectl svc -n default service-am-i-ready
kubectl get ep -n default
# you should see endpoints if the pods are running
kubectl get pods -n default am-i-ready -o wide
# we can see the ip address
kubectl describe pods -n default am-i-ready -o wide


TASK 5
kubectl config use-context k8s-c1-H
# sort pods by the metadata
 kubectl get pods -A --sort-by=metadata.creationTimestamp

echo ' kubectl get pods -A --sort-by=metadata.creationTimestamp
' > /opt/course/find_pods.sh
# sort by metauid
kubectl get nodes -A --sort-by=metauid


TASK 6
create a persistent volument of 2 gb rw no storage class defined

TASK 9
kubectl config use-context k8s-c2-AC
ssh cluster-master1  # master node
k get pods -n kube-system | grep -i kube-scheduler
cd /etc/kubernetes/manifests
mv ./kube.scheduler.yaml ../
kubectl run manual-schedule --image=httpd:2.22.01-alpine 
kubectl get po mannual-schedule

lets schedule that pod on cluster cluster2.master1 and make sure it is running
mv ../kube.scheduler.yaml ./
k get po -n kube-system | grep -i kube-scheduler
k get po -n kube-system --watch | grep -i kube-scheduler
k get po manual-schedule

## create a second pod- worker node
cd ../../../../
k run manual-scheduler2 --image=httpd:2.4-alpine  --dry-run=client -o yaml 

k run manual-scheduler2 --image=httpd:2.4-alpine  --dry-run=client -o yaml > 9.yaml
vi 9.yaml 
  edit specs: 
   add nodeName: cluster2-worker1
k get pods manyal-scheduler2 -o wide --watch ===# verifification that it is running 


,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.,....................................................................
TASK 21 
CREATE STATIC POD named my-static-pod-pod in namespace default on cluster3-master1
it shoudl be of image nginx:1.16-alpine and have resource requests for 10m cpu and 20m memory.

create a nodeport service NAMED my-static-pod-service which exposes the static pod on port 80 and check if it has 
Endpoints and if its reachable through the cluster3-master1 internal ip address,
connect to the internal nodes ips from your main terminal

kubectl config use-context K8s-c3-CCC
ssh cluster3-master1
cd /etc/kubernetes/manifests/
kubectl run my-static-pod-pod -n default --image=nginx:1.16-alpine --dry-run=client -o yaml
vi my-static-pod-pod.yaml
##resource requests on memory and cpu
resources:
  requests:
    memory: "10mu"
    cpus; "20mu"

kubectl get pods -n default
kubectl describe pods my-static-pod-pod-cluster3-master1
kubectl expose pod -n default my-static-pod-pod-cluster3-master1 --port=80 --type=nodePort --dry-run=client -0 yaml
kubectl expose pod -n default my-static-pod-pod-cluster3-master1 --name=static-pod-service --port=80 --type=nodePort --dry-run=client -0 yaml

# we can execute directly on the terminal without the --dry-run=client-o yaml 
so we get static-pod-service is now exposed
#to verify we use
kubectl get svc -n default
kubectl get ep -n default
kubectl get pods -n defaultmy-static-pod-pod-cluster3-master1 -o wide
lets assume the ip add of the node is 10.32.0.4
lets ping it 
kubectl -n default exec -it my-static-pod-pod-cluster3-master1 --ping 10.32.0.4

we should work on ndlookup 
